{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabelog_scraping_v.0.0.0.2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMORooJ1DdbnF4NueTqpKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yk-mt12/project-exercise/blob/master/tabelog_scraping_v_0_0_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HhHx-D-8AU_"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "class TabelogScraping:\n",
        "    \"\"\"\n",
        "    食べログから店名、ジャンル、評価点、口コミ件数、最新20件の口コミを取得する\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_num):\n",
        "        \"\"\"\n",
        "        インスタンス化した時点でスクレイピングを開始する\n",
        "        食べログ（検索条件：東京都、バー、ランキング順）にアクセスする\n",
        "        取得するページ件数は1ページ目からmax_numページ目までとする。\n",
        "        \"\"\"\n",
        "        # スクレイピング対象となるお店のタグ\n",
        "        self.TAGS = {'ダイニングバー', '和食', '日本料理',\n",
        "                     'パスタ', 'ハンバーグ', '中華料理', 'カフェ', 'ラウンジ'}\n",
        "        # スクレイピング対象となる評価点の下限値\n",
        "        self.LOWER_LIMIT = 3.0\n",
        "        # 口コミのURLへの不足部分\n",
        "        self.RVW = 'dtlrvwlst/'\n",
        "        # 取得したデータ一覧の保存場所\n",
        "        self.RESULT_PATH = './'\n",
        "        # 取得したデータ一覧名\n",
        "        self.RESULT_NAME = 'data_summary.csv'\n",
        "        # 取得した口コミデータフレームの列名\n",
        "        self.COLUMNS = ['store_id', 'store_name',\n",
        "                        'genre', 'rate', 'review_cnt', 'review']\n",
        "        # 検索するページ上限数\n",
        "        self.max_num = 2\n",
        "        self.shop_id = ''\n",
        "        self.id_num = 1\n",
        "        self.df = pd.DataFrame(columns=self.COLUMNS)\n",
        "\n",
        "        try:\n",
        "            first_time = time.time()\n",
        "            for page_num in range(1, self.max_num):\n",
        "                if page_num == 1:\n",
        "                    # ランキング1ページ目\n",
        "                    response = self.connect_url(\n",
        "                        'https://tabelog.com/osaka/rstLst/BC25/?SrtT=rt&Srt=D&sort_mode=1')\n",
        "                else:\n",
        "                    # ランキング2ページ目以降\n",
        "                    response = self.connect_url(\n",
        "                        'https://tabelog.com/osaka/rstLst/BC25/' + str(page_num) + '/?SrtT=rt&Srt=D&sort_mode=1')\n",
        "\n",
        "                # HTML解析用の変数\n",
        "                soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "                # 一覧ページから以下の情報を取得する\n",
        "                # ジャンルの取得\n",
        "                genre_info = [genre.text.rstrip() for genre in soup.find_all(\n",
        "                    'div', class_='list-rst__area-genre cpy-area-genre')]\n",
        "                # 評価点の取得\n",
        "                rate_list = [float(rate.text) for rate in soup.find_all(\n",
        "                    'span', class_='c-rating__val c-rating__val--strong list-rst__rating-val')]\n",
        "                # 口コミ件数の取得\n",
        "                count_list = [count.text for count in soup.find_all(\n",
        "                    'em', class_='list-rst__rvw-count-num cpy-review-count')]\n",
        "                # 店名の取得\n",
        "                raw_data = soup.find_all(\n",
        "                    'a', class_='list-rst__rst-name-target cpy-rst-name js-ranking-num')\n",
        "                shop_list = [name.text for name in raw_data]\n",
        "                shop_url_list = [shop_url.get('href') for shop_url in raw_data]\n",
        "                print(shop_url_list)\n",
        "\n",
        "                for i in range(20):\n",
        "                    # 何軒分のレビューを取得するか決定\n",
        "                    # i == 5 -> 4件目\n",
        "                    if i == 5:\n",
        "                        break\n",
        "\n",
        "                    genre_tags = set(\n",
        "                        re.split('[\\\\s/、]+', genre_info[i].strip())[2:])\n",
        "                    # ジャンル文字列が「バー、ワインバー、バー・お酒（その他）、ラウンジ、カフェ、ダイニングバー」の部分集合の場合、\n",
        "                    # かつ評価点が「3.０以上」の場合、かつ、口コミ件数が１件以上存在する場合に処理を続ける\n",
        "                    if not (rate_list[i] >= self.LOWER_LIMIT and int(count_list[i]) > 100):\n",
        "                        continue\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    self.shop_id = str(self.id_num).zfill(5)\n",
        "\n",
        "                    # 取得するreviewの詳細ページ番号\n",
        "                    reviewPageCount = 1\n",
        "                    # 取得するreviewの詳細ページの最大ページ数\n",
        "                    maxReviewPageCount = 7\n",
        "\n",
        "                    print(\"ループ回数\", i)\n",
        "\n",
        "                    while reviewPageCount < maxReviewPageCount:\n",
        "                        # お店のURLに遷移し、口コミを20件(1ページ分)取得する\n",
        "                        if reviewPageCount == 1:\n",
        "                            response = self.connect_url(\n",
        "                                shop_url_list[i] + self.RVW)\n",
        "                        else:\n",
        "                            response = self.connect_url(\n",
        "                                shop_url_list[i] + self.RVW + 'COND-0/smp1/?smp=1&lc=0&rvw_part=all&PG=' + str(reviewPageCount))\n",
        "\n",
        "                        print(\"ショップURL\", shop_url_list[i], response)\n",
        "                        print(\"レビューページカウント\", reviewPageCount, response)\n",
        "                        reviewPageCount += 1\n",
        "\n",
        "                        soup = BeautifulSoup(response.text, 'lxml')\n",
        "                        review_url_list = soup.find_all(\n",
        "                            'div', class_='rvw-item js-rvw-item-clickable-area')\n",
        "\n",
        "                        # -------------------- test --------------------\n",
        "                        # 1ページで取得する口コミの数\n",
        "                        loopCount = 0\n",
        "\n",
        "                        # 各口コミページに遷移し、最新の口コミを取得する\n",
        "                        for url in review_url_list:\n",
        "                            review_detail_url = 'https://tabelog.com' + \\\n",
        "                                url.get(\n",
        "                                    'data-detail-url')  # 各レビューの詳細ページのリンクを取得している\n",
        "                            response = self.connect_url(review_detail_url)\n",
        "                            soup = BeautifulSoup(response.text, 'lxml')\n",
        "                            review = soup.find_all(\n",
        "                                'div', class_='rvw-item__rvw-comment')\n",
        "                            review = review[0].p.text.strip()\n",
        "                            # pandasのデータフレームに格納\n",
        "                            self.add_df(shop_list[i], genre_info[i].strip(\n",
        "                            ), rate_list[i], count_list[i], review)\n",
        "\n",
        "                            # -------------------- test --------------------\n",
        "                            # 各店舗に対してレビューを6件取得できていたらOK\n",
        "                            # 1ページに対して3件が2ページ分\n",
        "\n",
        "                    # 1つのお店のスクレイピングが終了したら、その結果を保存する\n",
        "                    self.write_result(\n",
        "                        shop_list[i], genre_info[i].strip(), rate_list[i], count_list[i])\n",
        "\n",
        "                    process_time = time.time() - start_time\n",
        "                    print('{}件目完了：　処理時間：{:.3f}秒'.format(\n",
        "                        self.id_num, process_time))\n",
        "\n",
        "                    self.id_num += 1\n",
        "\n",
        "                    if self.id_num == 5:\n",
        "                        break\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            print(e)\n",
        "        except requests.exceptions.ConnectTimeout as e:\n",
        "            print(e)\n",
        "        finally:\n",
        "            end_time = time.time() - first_time\n",
        "            print('終了、処理時間：{:.3f}秒'.format(end_time))\n",
        "\n",
        "    def next_btn_click(driver, page_counter):\n",
        "        return False\n",
        "\n",
        "    def connect_url(self, target_url):\n",
        "        \"\"\"\n",
        "        対象のURLにアクセスする関数\n",
        "        アクセスできない等のエラーが発生したら例外を投げる\n",
        "        \"\"\"\n",
        "        # 接続確立の待機時間、応答待機時間を10秒とし、それぞれの値を超えた場合は例外が発生（ConnectTimeout）\n",
        "        data = requests.get(target_url, timeout=10)\n",
        "        data.encoding = data.apparent_encoding\n",
        "        # アクセス過多を避けるため、2秒スリープ\n",
        "        time.sleep(5)\n",
        "\n",
        "        # レスポンスのステータスコードが正常(200番台)以外の場合は、例外を発生させる(HTTPError)\n",
        "        if data.status_code == requests.codes.ok:\n",
        "            return data\n",
        "        else:\n",
        "            data.raise_for_status()\n",
        "\n",
        "    def write_result(self, name, genre, rate, count):\n",
        "        \"\"\"\n",
        "        スクレイピング対象となるお店の一覧をログとして出力する関数\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(self.RESULT_PATH, self.RESULT_NAME)\n",
        "        with open(file_path, mode='a', encoding='utf-8') as f:\n",
        "            f.write('{}, {}, {}, 評価：{}, 口コミ：{}件\\n'.format(\n",
        "                self.shop_id, name, genre, rate, count))\n",
        "\n",
        "    def add_df(self, name, genre, rate, count, comment):\n",
        "        \"\"\"\n",
        "        取得した口コミデータをデータフレームに格納する関数\n",
        "        \"\"\"\n",
        "        se = pd.Series([self.shop_id, name, genre, rate,\n",
        "                       count, comment], self.COLUMNS)\n",
        "        self.df = self.df.append(se, self.COLUMNS)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = TabelogScraping(1)\n",
        "    print(result.df)\n",
        "    result.df.to_csv(\"./review.csv\")\n"
      ]
    }
  ]
}